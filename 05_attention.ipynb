{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention models for Time Series\n",
    "\n",
    "* Author: Romain Tavenard (@rtavenar)\n",
    "* License: CC-BY-NC-SA\n",
    "\n",
    "A notebook from a course on Machine Learning for Time Series at ENSAI.\n",
    "One can find lecture notes for this course [there](https://rtavenar.github.io/ml4ts_ensai/).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy seq2seq task\n",
    "\n",
    "For a start, have a look at the data produced by the following `gen_data` function.\n",
    "The task at hand is to predict the output sequence from its corresponding input one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def make_series(positions, heights, shapes, size):\n",
    "    series = np.zeros((size, ))\n",
    "    for p, h, s in zip(positions, heights, shapes):\n",
    "        if s == \"r\":\n",
    "            series[p-5:p+5] = h\n",
    "        else:\n",
    "            series[p-5:p] = np.linspace(start=0., stop=h, num=5)\n",
    "            series[p:p+5] = np.linspace(stop=0., start=h, num=5)\n",
    "    return series.reshape((-1, 1))\n",
    "    \n",
    "\n",
    "def gen_data(n_samples, noise_level=.1):\n",
    "    inputs, outputs = [], []\n",
    "    shapes = np.array([\"t\", \"r\"] * 2)\n",
    "    n_shapes = len(shapes)\n",
    "    \n",
    "    sz = 100\n",
    "    region_width = sz // n_shapes\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        np.random.shuffle(shapes)\n",
    "        index_high_t = np.random.choice(np.where(shapes == \"t\")[0], size=1)[0]\n",
    "        index_high_r = np.random.choice(np.where(shapes == \"r\")[0], size=1)[0]\n",
    "        \n",
    "        base_input_series = np.random.randn(sz, 1) * noise_level\n",
    "        heights = []\n",
    "        positions = []\n",
    "        for idx_shape in range(n_shapes):\n",
    "            pos = idx_shape * region_width + np.random.randint(low=5, high=region_width - 5)\n",
    "            height = np.random.rand(1)[0] * 10.\n",
    "            if idx_shape in [index_high_r, index_high_t]:\n",
    "                height += 10.\n",
    "            heights.append(height)\n",
    "            positions.append(pos)\n",
    "        input_series = np.random.randn(sz, 1) * noise_level + make_series(positions, heights, shapes, sz)\n",
    "        \n",
    "        normalized_heights = np.array(heights)\n",
    "        for s in [\"t\", \"r\"]:\n",
    "            normalized_heights[shapes == s] = normalized_heights[shapes == s].mean()\n",
    "        output_series = np.random.randn(sz, 1) * noise_level + make_series(positions, normalized_heights, shapes, sz)\n",
    "        inputs.append(input_series)\n",
    "        outputs.append(output_series)\n",
    "            \n",
    "    return np.array(inputs), np.array(outputs)\n",
    "\n",
    "np.random.seed(0)\n",
    "inputs, outputs = gen_data(100)\n",
    "test_inputs, test_outputs = gen_data(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question #1.** Visualize the first 6 input/output pairs (each in one subfigure). Can you guess how the height of output motifs is computed?\n",
    "Why would this data better suited to attention-based models than convolutional ones, for example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question #2.** Check how a model made of a stack of 5 convolutional layers (use 64 filters in all layers except the last one) performs on this task. What loss function should you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question #3.** Below is the definition of a multi-head self attention layer class. Use this class to replace the 3rd convolution in your model above by a **single-head self-attention layer** that would output a 64-dimensional time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "class MultiHeadSelfAttention(MultiHeadAttention):\n",
    "    def __init__(self, num_heads, key_dim, dropout=0.):\n",
    "        super().__init__(num_heads, key_dim, dropout=dropout)\n",
    "\n",
    "    def call(self, x, return_attention_scores=False):\n",
    "        return super().call(x, x, return_attention_scores=return_attention_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question #4.** Compare both models in terms of validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question #5.** Use test data for qualitative inspection of the produced outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question #6.** Use the code below to visualize average attention scores for the first 3 test series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def visualize_average_attention_scores(model, index_of_attention_layer, time_series):\n",
    "    sub_model = Sequential(model.layers[:index_of_attention_layer])\n",
    "    input_features = sub_model(time_series)\n",
    "    att_layer = model.layers[index_of_attention_layer]\n",
    "    output_tensor, weights = att_layer(input_features, return_attention_scores=True)\n",
    "\n",
    "    plt.figure(figsize=(4*len(time_series), 4))\n",
    "    for idx, ts in enumerate(time_series):\n",
    "        plt.subplot(2, len(time_series), idx + 1)\n",
    "        plt.plot(ts.ravel())\n",
    "        plt.title(\"Input series\")\n",
    "        plt.subplot(2, len(time_series), len(time_series) + idx + 1)\n",
    "        plt.plot(weights[idx, 0].numpy().mean(axis=0))\n",
    "        plt.title(\"Average attention scores\")\n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention models for Time Series Classification\n",
    "\n",
    "In this section, you will design a model that relies on attention for the Trace dataset you already used in previous labs.\n",
    "\n",
    "**Question #7.** Load the Trace dataset, merge training and test data and shuffle the resulting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question #8.** Using only convolutions, self-attention and dense layers, design a first neural network architecture for the task at hand. You will use 20% of the data as validation (see the `validation_split` argument of the `model.fit` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question #9.** Sub-class the `Conv1D` layer to implement a residual layer that would compute its output as:\n",
    "\n",
    "$$\\text{ResidualConv}(x) = x + \\text{Conv}(x)$$\n",
    "\n",
    "where $\\text{Conv}(x)$ is a 1d convolution applied to the input series $x$.\n",
    "\n",
    "Why would it make sense to use such a layer in place of a convolutional one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question #10.** Replace convolutional layers by residual convolutional layers in your previous model and see if performance is improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IMongo",
   "language": "python",
   "name": "imongo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25f9a3951446179f6c2016b22a60b44495fe90f43bda7f3caedfe2c1a9cd31f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
